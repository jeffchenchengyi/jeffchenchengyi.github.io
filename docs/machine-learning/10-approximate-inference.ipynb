{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Approximate Inference [In Progress]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.1. Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1 Factorized distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2 Properties of factorized approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3 Example: The univariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.4 Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.2. Illustration: Variational Mixture of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 Variational distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 Variational lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3 Predictive density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.4 Determining the number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.5 Induced factorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.3. Variational Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 Variational distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 Predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3 Lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.4. Exponential Family Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.1 Variational message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.5. Local Variational Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.6. Variational Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6.1 Variational posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6.2 Optimizing the variational parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6.3 Inference of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.7. Expectation Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7.1 Example: The clutter problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7.2 Expectation propagation on graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Setup\n",
    "\n",
    "Given the following setup:\n",
    "\n",
    "- I.I.D. Observed data: $\\mathcal{X} = \\left(X^{(1)}, \\cdots, X^{(D)}\\right)$, where each $X^{(i)} = (x^{(i)}_{1}, x^{(i)}_{2}, \\cdots, x^{(i)}_{M})$\n",
    "- Unobserved latent data or missing values: $\\mathcal{Z}$\n",
    "- Vector of unknown parameters: $\\boldsymbol {\\theta }$\n",
    "- Likelihood function: $L({\\boldsymbol {\\theta }};\\mathcal{X} ,\\mathcal{Z} )=p(\\mathcal{X} ,\\mathcal{Z} \\mid {\\boldsymbol {\\theta }})$\n",
    "\n",
    "we have 2 tasks that are of interest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Find the MLE ($\\boldsymbol {\\theta}$ is parameter) / MAP ($\\boldsymbol {\\theta}$ is random variable) estimates of the model parameters $\\boldsymbol {\\theta_{\\rm{max}}}$ {cite}`Expectat45:online`:\n",
    "\n",
    "\\begin{align}\n",
    "    \\boldsymbol {\\theta_{\\rm{max}}} &= \\underset{\\boldsymbol {\\theta}}{\\operatorname{argmax}} \\log p(\\mathcal{X} \\mid \\boldsymbol{\\theta }) \\\\\n",
    "    &= \\underset{\\boldsymbol {\\theta}}{\\operatorname{argmax}} \\log \\sum_{z \\in \\mathcal{Z}} p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $p(\\mathcal{X} \\mid \\boldsymbol{\\theta })$ is known as the ***Evidence / Marginal Likelihood / Incomplete-data Likelihood*** and $p(\\mathcal{X} ,\\mathcal{Z} \\mid {\\boldsymbol {\\theta }})$ is known as the ***Complete-data likelihood***.\n",
    "\n",
    "- 2. Find the posterior over the latent variables $\\mathcal{Z}$, $p(\\mathcal{Z} \\mid \\mathcal{X}, \\boldsymbol {\\theta_{\\rm{max}} })$ {cite}`SVIPartI61:online`:\n",
    "\n",
    "\\begin{align}\n",
    "    p(\\mathcal{Z} \\mid \\mathcal{X}, \\boldsymbol {\\theta_{\\rm{max}} }) &= \\frac{p(\\mathcal{X}, \\mathcal{Z} \\mid \\boldsymbol {\\theta_{\\rm{max}} })}{\n",
    "\\sum_{z \\in \\mathcal{Z}} p(\\mathcal{X}, \\mathcal{Z} = z \\mid \\boldsymbol {\\theta_{\\rm{max}} })} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note} Notice that $\\sum_{z \\in \\mathcal{Z}}$ is a general summation to encompass all types of marginalization possible, i.e. can be a multidimensional summation $\\sum_{z_1 \\in Z_1}\\sum_{z_2 \\in Z_2}\\cdots\\sum_{z_M \\in Z_M}$ or integral(s) if $z$ is continuous or even a mix of summation and integration over different latent variables of interest.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this $p(\\mathcal{X} \\mid \\boldsymbol{\\theta }) = \\sum_{z \\in \\mathcal{Z}} p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }})\\,d\\mathcal{Z}$ is often intractable (e.g. if $\\mathcal{Z}$  is a sequence of events, so that the number of values grows exponentially with the sequence length, the exact calculation of the sum will be extremely difficult). Let's instead try to find a lower bound for it by expanding it {cite}`SVIPartI61:online`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\log p(\\mathcal{X} \\mid \\boldsymbol{\\theta }) \n",
    "    &= \\log \\sum_{z \\in \\mathcal{Z}} p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }}) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ideas from importance sampling, assume we have another variational distribution [approximate posterior distribution to $p({\\mathcal{Z}} \\mid {\\mathcal{X}}, {\\bf \\theta})$)], $q({\\mathcal{Z}} \\mid {\\bf \\phi})$, where $q({\\mathcal{Z}} \\mid {\\bf \\phi}) > 0$ whenever $p({\\mathcal{Z}}) = \\sum_{x \\in \\mathcal{X}} p({\\mathcal{X}} = x, {\\mathcal{Z}} \\mid {\\bf \\theta}) > 0$, and we rewite:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\log p(\\mathcal{X} \\mid \\boldsymbol{\\theta }) \n",
    "    &= \\log \\sum_{z \\in \\mathcal{Z}} p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }}) \\frac{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\\\\n",
    "    &= \\log \\operatorname {E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\left[\\frac{p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }})}{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\right] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Jensen's Inequality, given concave function $f(X)$ (e.g. $\\log$), $f\\operatorname {E}\\left[X\\right] \\geq \\operatorname {E}\\left[f(X)\\right]$ {cite}`Variatio28:online`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\log p(\\mathcal{X} \\mid \\boldsymbol{\\theta }) \n",
    "    &= \\log \\operatorname {E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\left[\\frac{p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }})}{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\right] \\\\\n",
    "    &\\geq \\operatorname {E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\left[\\log\\left(\\frac{p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }})}{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}\\right)\\right] \\\\\n",
    "    &= \\operatorname {E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\left[\\log p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }}) - \\log q({\\mathcal{Z} = z} \\mid {\\bf \\phi})\\right] \\\\\n",
    "    &= \\operatorname {E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\left[\\log p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }})\\right] - \\operatorname {E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\left[\\log q({\\mathcal{Z} = z} \\mid {\\bf \\phi})\\right] \\\\\n",
    "    &= \\underbrace{\\underbrace{\\operatorname {E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\left[\\log p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }})\\right]}_{\\text{Expected Complete-data Log Likelihood}} + \\underbrace{\\operatorname{H}\\left[\\log q({\\mathcal{Z}} \\mid {\\bf \\phi})\\right]}_{\\text{Entropy of Variational Dist.}}}_{\\text{ELBO / Negative Variational Free Energy } \\mathcal{L}(q({\\mathcal{Z}}\\mid {\\bf \\phi}))} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we get an ***Evidence Lower Bound (ELBO)*** (also known as the ***Negative Variational Free Energy***) on the $\\log$ Evidence. Instead of an inequality, we can get an exact equality of the form below by deriving the ELBO from rearranging the KL Divergence from our variational distribution (approximate posterior of latent variables) $q({\\mathcal{Z}} \\mid {\\bf \\phi})$ to our actual posterior over latent variables $p({\\mathcal{Z}} \\mid {\\mathcal{x}}, {\\bf \\theta})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation from ${\\rm KL}(q({\\mathcal{Z}} \\mid {\\bf \\phi}) \\mid\\mid p({\\mathcal{Z}} \\mid {\\mathcal{x}}, {\\bf \\theta}))$:\n",
    "\n",
    "\\begin{align}\n",
    "    {\\rm KL}(q({\\mathcal{Z}} \\mid {\\bf \\phi}) \\mid\\mid p({\\mathcal{Z}} \\mid {\\mathcal{x}}, {\\bf \\theta}))\n",
    "    &= \\operatorname{E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}\\left[\\log\\left(\\frac{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}{p({\\mathcal{Z} = z} \\mid {\\mathcal{x}}, {\\bf \\theta})}\\right)\\right] \\\\\n",
    "    &= \\operatorname{E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}\\left[\\log q({\\mathcal{Z} = z} \\mid {\\bf \\phi})\\right] - \\operatorname{E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}\\left[\\log p({\\mathcal{Z} = z} \\mid {\\mathcal{x}}, {\\bf \\theta})\\right] \\\\\n",
    "    &= \\operatorname{E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}\\left[\\log q({\\mathcal{Z} = z} \\mid {\\bf \\phi})\\right] - \\operatorname{E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}\\left[\\log p({\\mathcal{Z} = z}, {\\mathcal{x}} \\mid {\\bf \\theta})\\right] + \\operatorname{E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}\\left[\\log p({\\mathcal{x}} \\mid {\\bf \\theta})\\right] \\\\\n",
    "    &= -\\left[\\operatorname {E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})} \\left[\\log p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }})\\right] + \\operatorname{H}\\left[\\log q({\\mathcal{Z}} \\mid {\\bf \\phi})\\right]\\right] + \\operatorname{E}_{q({\\mathcal{Z} = z} \\mid {\\bf \\phi})}\\left[\\log p({\\mathcal{x}} \\mid {\\bf \\theta})\\right] \\\\\n",
    "    &= -\\mathcal{L}(q({\\mathcal{Z} = z}\\mid {\\bf \\phi})) + \\log p({\\mathcal{x}} \\mid {\\bf \\theta}) \\because \\text{Expectation is over latent variables }{\\mathcal{Z} = z}\\text{, which is independent of }{\\mathcal{x}} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\therefore \\log p({\\mathcal{x}} \\mid {\\bf \\theta}) \n",
    "    &= \\mathcal{L}(q({\\mathcal{Z}} \\mid {\\bf \\phi})) + {\\rm KL}(q({\\mathcal{Z}} \\mid {\\bf \\phi}) \\mid\\mid p({\\mathcal{Z}} \\mid {\\mathcal{x}}, {\\bf \\theta})) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\log p({\\mathcal{x}} \\mid {\\bf \\theta})$ is a constant, maximizing our ELBO / Negative Variational Free Energy will be equivalent to minimizing the ${\\rm KL}(q({\\mathcal{Z}} \\mid {\\bf \\phi}) \\mid\\mid p({\\mathcal{Z}} \\mid {\\mathcal{x}}, {\\bf \\theta}))$ (0 when $q({\\mathcal{Z}} \\mid {\\bf \\phi}) = p({\\mathcal{Z}} \\mid {\\mathcal{x}}, {\\bf \\theta})$), making our variational approximation as close as possible to the actual posterior over latents. After this procedure, our 2 tasks will look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Find the MLE (${\\bf\\theta}, {\\bf\\phi}$ are parameters) / MAP (${\\bf\\theta}, {\\bf\\phi}$ are random variables) estimates of the model parameters ${\\bf \\theta_{\\rm{max}}}, {\\bf \\phi_{\\rm{max}}}$ by maximizing the ELBO:\n",
    "\n",
    "\\begin{align}\n",
    "    {\\bf\\theta_{\\rm{max}}} &= \\underset{\\boldsymbol {\\theta}}{\\operatorname{argmax}} \\log p(\\mathcal{X} \\mid \\boldsymbol{\\theta }) \\\\\n",
    "    {\\bf\\theta_{\\rm{max}}}, {\\bf\\phi_{\\rm{max}}} &\\approx \\underset{{\\bf \\theta}, {\\bf \\phi}}{\\operatorname{argmax}} \\mathcal{L}(q({\\mathcal{Z}} \\mid {\\bf \\phi})) \\\\\n",
    "    &= \\underset{{\\bf \\theta}, {\\bf \\phi}}{\\operatorname{argmax}} \\operatorname {E}_{q({\\mathcal{Z}} = z \\mid {\\bf \\phi})} \\left[\\log p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }})\\right] - \\operatorname{H}\\left[\\log q({\\mathcal{Z}} \\mid {\\bf \\phi})\\right]  \\\\\n",
    "\\end{align}\n",
    "\n",
    "In maximizing the ELBO, the first term, Expected Complete-data Log Likelihood, encourages the MLE / MAP estimates of the model parameters to be \n",
    "\n",
    "- 2. Find the posterior over the latent variables $\\mathcal{Z}$, $p(\\mathcal{Z} \\mid \\mathcal{X}, \\boldsymbol {\\theta_{\\rm{max}} })$ {cite}`SVIPartI61:online`:\n",
    "\n",
    "\\begin{align}\n",
    "    p(\\mathcal{Z} \\mid \\mathcal{X}, \\boldsymbol {\\theta_{\\rm{max}} }) &\\approx q({\\mathcal{Z}} \\mid {\\bf \\phi}) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 1: Expectation-Maximization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM algorithm seeks to find the MLE of the evidence / marginal likelihood / incomplete-data likelihood by iteratively applying these two steps {cite}`Expectat45:online`:\n",
    "\n",
    "- 1. Expectation step (E step): Set the approximate posterior / variational distribution $q({\\mathcal{Z}}\\mid {\\bf \\phi}) = p(\\mathcal{Z} \\mid \\mathcal{X}, \\boldsymbol {\\theta_{t} })$, where $\\bf \\theta_{t}$ are the previous M-step estimates of $\\bf \\theta$, this way the ${\\rm KL}(q({\\mathcal{Z}} \\mid {\\bf \\phi}) \\mid\\mid p({\\mathcal{Z}} \\mid {\\mathcal{x}}, {\\bf \\theta})) = 0$ and $\\log p({\\mathcal{x}} \\mid {\\bf \\theta}) = \\mathcal{L}(p({\\mathcal{Z}} \\mid {\\mathcal{x}}, {\\bf \\theta_{t}}))$. Our objective is then to \n",
    "\n",
    "    - A. Calculate the posterior over latent variables $p(\\mathcal{Z} \\mid \\mathcal{X} ,{\\boldsymbol {\\theta }}^{(t)})$ and \n",
    "    \n",
    "    - B. Calculate $Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})$ (Expected Complete data Log Likelihood):\n",
    "\n",
    "\\begin{align}\n",
    "    Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)}) &= \\operatorname {E} _{p(\\mathcal{Z} = z \\mid \\mathcal{X} ,{\\boldsymbol {\\theta }}^{(t)})}\\left[\\log L({\\boldsymbol {\\theta }};\\mathcal{X} ,\\mathcal{Z} = z )\\right]\\, \\\\\n",
    "    &= \\operatorname {E} _{p(\\mathcal{Z} = z \\mid \\mathcal{X} ,{\\boldsymbol {\\theta }}^{(t)})}\\left[\\log p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }}) \\right]\\, \\\\\n",
    "    &= \\sum_{z \\in \\mathcal{Z}} p(\\mathcal{Z} = z \\mid \\mathcal{X} ,{\\boldsymbol {\\theta }}^{(t)}) \\log p(\\mathcal{X} ,\\mathcal{Z} = z \\mid {\\boldsymbol {\\theta }}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Notice that the only thing that is missing from $Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})$ compared to the ELBO is the entropy of the approximate posterior distribution $\\operatorname{H}\\left[\\log q({\\mathcal{Z}} \\mid {\\bf \\phi})\\right]$.\n",
    "\n",
    "- 2. Maximization step (M step): Find the parameters that maximize $ Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})$:\n",
    "\n",
    "\\begin{align} \n",
    "    {\\boldsymbol {\\theta }}^{(t+1)} &= {\\underset {\\boldsymbol {\\theta }}{\\operatorname {arg\\,max} }}\\ Q({\\boldsymbol {\\theta }}\\mid {\\boldsymbol {\\theta }}^{(t)})\\,\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 01 - Gaussian Mixture Model {cite}`Expectat31:online`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 02 - Latent Dirichlet Allocation Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 03 - Hidden Markov Model {cite}`10.5555/1162264`{cite}`hmmbaumw42:online`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HMM Setup**\n",
    "\n",
    "Given the following setup:\n",
    "\n",
    "- I.I.D. Observed data: $\\mathcal{X} = \\left(X^{(1)}, \\cdots, X^{(D)}\\right)$, \n",
    "    \n",
    "    - where each $X^{(i)} = (x^{(i)}_{1}, x^{(i)}_{2}, \\cdots, x^{(i)}_{T})$, \n",
    "    \n",
    "    - where each $x^{(i)}_{t}$ can take on 1 of $\\left{1, 2, \\cdots K \\right}$ values\n",
    "\n",
    "- Unobserved latent data or missing values: $\\mathcal{Z} = \\left(Z^{(1)}, \\cdots, Z^{(D)}\\right)$, \n",
    "    \n",
    "    - where each $Z^{(i)} = (z^{(i)}_{1}, z^{(i)}_{2}, \\cdots, z^{(i)}_{T})$, \n",
    "    \n",
    "    - where each $z^{(i)}_{t}$ can take on 1 of $\\left{1, 2, \\cdots N \\right}$ values\n",
    "\n",
    "- Unknown parameters: $\\mathbf{\\theta} = \\left{A, B, \\pi\\right}$\n",
    "    \n",
    "    - where $\\pi = (\\pi_1, \\pi_2, \\cdots, \\pi_N)$ is the $1 \\times N$ initial state  matrix,\n",
    "    \n",
    "        - where each $\\pi_{i} = p(z_{1} = i)$\n",
    "    \n",
    "    - where $A$ is the $N \\times N$ state transmission matrix\n",
    "    \n",
    "        - where each $A_{ij} = p(z_{t+1} = j \\mid z_{t} = i)$\n",
    "    \n",
    "    - where $B$ is the $N \\times K$ state transmission matrix\n",
    "    \n",
    "        - where each $B_{i}(j) = p(x_{t} = j \\mid z_{t} = i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E-step**\n",
    "\n",
    "\\begin{align}\n",
    "    \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 2: Markov Chain Monte Carlo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 3: Mean-field Approximation Variational Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    {\\rm ELBO} &\\equiv \\mathbb{E}_{q({\\bf z} \\mid {\\bf \\phi})} \\left [\n",
    "\\log p({\\bf x}, {\\bf z} \\mid {\\bf \\theta}) - \\log q({\\bf z} \\mid {\\bf \\phi})\n",
    "\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 01 - Gaussian Mixture Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 02 - Latent Dirichlet Allocation Topic Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 03 - Hidden Markov Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 4: Black-box Variational Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    {\\rm ELBO} &\\equiv \\mathbb{E}_{q({\\bf z} \\mid {\\bf \\phi})} \\left [\n",
    "\\log p({\\bf x}, {\\bf z} \\mid {\\bf \\theta}) - \\log q({\\bf z} \\mid {\\bf \\phi})\n",
    "\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 01 - Gaussian Mixture Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 02 - Latent Dirichlet Allocation Topic Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 03 - Hidden Markov Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 04 - Bayesian Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
