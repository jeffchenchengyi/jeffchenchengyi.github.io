{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "external-video",
   "metadata": {},
   "source": [
    "# 1. Linear Algebra [In Progress]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-ghost",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-alexander",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2. Eigenvalue Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-shark",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.3. Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-portland",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.4. Symmetric Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-africa",
   "metadata": {},
   "source": [
    "```{tip} Theorem: $X^\\top X \\succeq 0 \\forall X$ {cite}`2IsBáµ€BAl41:online`\n",
    "\n",
    "Is $X^\\top X$ always $\\succeq 0$? Yes, and $X^\\top X \\succ 0$ when $X$ has linearly independent column vectors (invertible) \n",
    "\n",
    "Proof:\n",
    "\n",
    "Suppose $X \\in \\mathbb{R}^{m \\times n}$ matrix and $y \\in \\mathbb{R}^n$ vector: \n",
    "\n",
    "\\begin{align}\n",
    "    y^\\top X^\\top X y &= {\\left( \\underbrace{Xy}_{z} \\right)}^\\top \\underbrace{Xy}_{z} \\\\\n",
    "    &= z^\\top z \\begin{cases}\n",
    "        &= 0 \\text{ if } y \\in N(X) \\left(y\\text{ is in the nullspace of } X \\right) \\\\\n",
    "        &> 0 \\text{ if } y \\not\\in N(X) \\left(y\\text{ is not in the nullspace of } X \\right) \\\\\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-control",
   "metadata": {},
   "source": [
    "```{tip} Theorem: $X^\\top A X \\succeq 0 \\forall X$ {cite}`2ForaPos52:online`\n",
    "\n",
    "Is $X^\\top A X$ always $\\succeq 0$? Given that $A \\succ 0$, yes. $X^\\top A X \\succ 0$ if $X$ has linearly independent column vectors (invertible), and $X^\\top A X \\succeq 0$ is $X$ has linearly dependent column vectors.\n",
    "\n",
    "Proof:\n",
    "\n",
    "Suppose $X \\in \\mathbb{R}^{m \\times n}$ matrix, $A \\in \\mathbb{R}^{m \\times m} \\succ 0$ matrix, and $y \\in \\mathbb{R}^n$ vector: \n",
    "\n",
    "\\begin{align}\n",
    "    y^\\top X^\\top A X y &= {\\left( \\underbrace{Xy}_{z} \\right)}^\\top A \\underbrace{Xy}_{z} \\\\\n",
    "    &= z^\\top A z \\begin{cases}\n",
    "        &= 0 \\text{ if } y \\in N(X) \\left(y\\text{ is in the nullspace of } X \\right) \\\\\n",
    "        &> 0 \\text{ if } y \\not\\in N(X) \\left(y\\text{ is not in the nullspace of } X \\right) \\\\\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-carrier",
   "metadata": {},
   "source": [
    "### 1.4.1. Quadratic Form Minimization\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\min\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-investment",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.5. Projections \n",
    "\n",
    "{cite}`Aproject8:online`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-deployment",
   "metadata": {},
   "source": [
    "### 1.5.1. Projections onto a subspace is a linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-zimbabwe",
   "metadata": {},
   "source": [
    "Let $V$ be a subspace in $\\mathbb{R}^{n}$\n",
    "\n",
    "Let $B = \\left\\{\\vec{b}_1, \\vec{b}_2, \\cdots, \\vec{b}_k\\right\\}$ is a basis for $V$, and $\\therefore \\forall \\vec{a} \\in V \\Rightarrow \\vec{a} = y_1\\vec{b}_1 + y_2\\vec{b}_2 + \\cdots + y_k\\vec{b}_k$\n",
    "\n",
    "Let $A$ be the matrix of the basis vectors of $B$,\n",
    "\n",
    "\\begin{align}\n",
    "    A\\vec{y} &=\n",
    "    \\begin{bmatrix}\n",
    "        \\vert & \\vert & & \\vert \\\\\n",
    "        \\vec{b}_1 & \\vec{b}_2 & \\cdots & \\vec{b}_k   \\\\\n",
    "        \\vert & \\vert & & \\vert \\\\\n",
    "    \\end{bmatrix} \n",
    "    \\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        y_k \\\\\n",
    "    \\end{bmatrix}, \\text{ for some } \\vec{y} \\in \\mathbb{R}^{k} \\\\\n",
    "    &= y_1\\vec{b}_1 + y_2\\vec{b}_2 + \\cdots + y_k\\vec{b}_k \\\\\n",
    "    &= \\vec{a}  \\\\\n",
    "\\end{align}\n",
    "\n",
    "A projection of an arbitrary vector $\\vec{x} \\in \\mathbb{R}^{n}$ into the subspace of $V$ is denoted as ${Proj}_{V}{\\vec{x}} \\in V$.\n",
    "\n",
    "Since any vector in the subspace of $V$ can be formulated as a linear combination of the basis vectors, we get\n",
    "\n",
    "\\begin{align}\n",
    "    {Proj}_{V}{\\vec{x}} &= A\\vec{y} \\\\\n",
    "\\end{align}\n",
    "\n",
    "We also know that $\\vec{x} = \\underbrace{{Proj}_{V}{\\vec{x}}}_{\\in V} + \\underbrace{{Proj}_{V^\\complement}{\\vec{x}}}_{\\in V^\\complement}$.\n",
    "\n",
    "Furthermore, since $V$ is the same as the column space of $A$, the matrix basis vectors that spans $V$, the complement of $V$ is actually the nullspace of $A^\\top$:\n",
    "\n",
    "\\begin{align}\n",
    "    C(A) &= V \\\\\n",
    "    V^\\complement &= {(C(A))}^\\complement = N(A^\\top) \\\\\n",
    "\\end{align}\n",
    "\n",
    "From the definition of the nullspace,\n",
    "\n",
    "````{margin}\n",
    "```{tip} Theorem\n",
    "\n",
    "If a matrix $A$ has linearly independent columns (invertible via Invertible Matrix Theorem), then $A^\\top A$ is always invertible {cite}`linearal7:online`.\n",
    "\n",
    "Proof:\n",
    "\n",
    "By Invertible Matrix Theorem,\n",
    "\n",
    "\\begin{align}\n",
    "    A\\vec{x} &= 0 \\Rightarrow \\vec{x} = 0 \\\\\n",
    "    \\therefore N(A^\\top) &= \\vec{0}, A^\\top A\\vec{x} = 0 \\iff \\vec{x} = \\vec{0} \\\\\n",
    "\\end{align}\n",
    "\n",
    "```\n",
    "````\n",
    "\n",
    "\\begin{align}\n",
    "    A^\\top \\left({Proj}_{V^\\complement}{\\vec{x}}\\right) &= 0 \\\\\n",
    "    A^\\top \\left(\\vec{x} - {Proj}_{V}{\\vec{x}}\\right) &= 0 \\\\\n",
    "    A^\\top \\vec{x} - A^\\top {Proj}_{V}{\\vec{x}} &= 0 \\\\\n",
    "    A^\\top \\vec{x} - A^\\top A\\vec{y} &= 0 \\\\\n",
    "    A^\\top \\vec{x} &= A^\\top A\\vec{y} \\\\\n",
    "    \\vec{y} &= \\underbrace{{(A^\\top A)}^{-1}}_{\\text{Invertible } \\because A \\text{ has linearly independent columns}} A^\\top \\vec{x} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    {Proj}_{V}{\\vec{x}} &= A {(A^\\top A)}^{-1} A^\\top \\vec{x} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Hence, we see that projections onto a subspace is a linear transformation.\n",
    "\n",
    "Notice also, that if $A$ was instead a vector $\\vec{w} \\in \\mathbb{R}^{n}$, our formula in {eq}`13` reduces to the normal vector projection formula:\n",
    "\n",
    "\\begin{align}\n",
    "    {Proj}_{\\vec{w}}{\\vec{x}} \n",
    "    &= \\vec{w} \\underbrace{{(\\vec{w}^\\top \\vec{w})}^{-1}}_{\\text{Scalar}} \\underbrace{\\vec{w}^\\top \\vec{x}}_{\\text{Scalar}} \\\\\n",
    "    &= \\frac{\\vec{w}^\\top \\vec{x}}{{\\lVert \\vec{w} \\rVert}^2}\\vec{w} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-keyboard",
   "metadata": {},
   "source": [
    "### 1.5.2. Applications of Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-landscape",
   "metadata": {},
   "source": [
    "#### 1.5.2.1 Geometric Interpretation of Ordinary Least Squares Regression \n",
    "\n",
    "{cite}`2Leastsq52:online, Ordinary7:online`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-survey",
   "metadata": {},
   "source": [
    "Given a dataset $\\mathbf{X} = \\left\\{\\mathbf{x}_{n} \\vert \\mathbf{x}_{n} \\in \\mathbb{R}^{D}, n \\in 1, \\cdots N \\right\\}$ and response variables $\\mathbf{y} = \\left(y_1, \\cdots, y_N\\right)$, we model the following:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{y} &= \\mathbf{X}\\beta + \\epsilon \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta$ is a vector of coefficients of each feature / covariate / predictor / column in the data matrix $\\mathbf{X}$ and $\\epsilon$ is a vector of unobserved error random variables which explain the $\\mathbf{y}$ outside of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-painting",
   "metadata": {},
   "source": [
    "Notice that in a perfect world, where $\\mathbf{X}$ is the only source of variation in $\\mathbf{y}$, the linear regression / OLS model reduces to a regular inhomogeneous system of linear equations:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{y} &= \\mathbf{X}\\beta \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-seattle",
   "metadata": {},
   "source": [
    "However, $\\mathbf{X}$ is often not a complete set of predictors for $\\mathbf{y}$ and hence, we will most definitely get $\\epsilon$. We can however, try to minimize $\\epsilon$ by finding a best fit $\\beta$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\beta} &= \\arg\\underset{\\beta}{\\min} {\\lVert \\mathbf{y} - \\mathbf{X} \\beta \\rVert}_{2} \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-speaker",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "```{bibliography} ../references.bib\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
