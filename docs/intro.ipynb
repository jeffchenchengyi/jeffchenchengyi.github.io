{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Psi \\Phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./images/header.jpg\n",
    ":name: Chengyi (Jeff) Chen\n",
    "\n",
    "Hi, I'm Jeff and $\\Psi \\Phi$ (pronounced \"sci-fi\") is my technical blog dedicated to connecting seemingly disparate data science concepts.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this site, \"Machine Learning\", seeks to summarize the majority of material from \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop (best resource for learning probabilistic machine learning I can find) and add a few code snippets here and there that'll primarily serve to facilitate my own understanding of the material. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chapters only include skeletons of the topics inside the book right now because I've been juggling between full-time study and full-time work the past semester and recently revamped my blog. I'll aim to complete them by the end of 2022, unless a different interest piques my interest. Other than the topics in the book, I'll also include a few additional topics I'm interested in such as Contextual Bandits, Genetic Algorithms, and domain-specific topics such as Natural Language Processing and Computer Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, my interests lie within the realm of probabilistic machine learning, which all started when I tried to understand Variational Auto-Encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second and third parts of the blog, \"Personal Projects\" and \"Featured Course Work & Extra-curriculars\" contains some of the important course work that I've enjoyed while completing my Bachelor's in Computer Science and Business Administration and Master's in Analytics from 2017 - 2021. If you're looking for more traditional optimization-related course work, it'll primarily be found inside ISE-530: Optimization Analytics, while some of the more quant-finance-focused course work wiL be found in ISE-537: Financial Analytics, though solving optimization problems exist in practically all the course-work I've featured in this blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} No Free Lunch Theorem\n",
    ":class: tip\n",
    "\n",
    "If you're not familiar with the NFL Theorem, it basically states that there isn't a single \"best\" machine-learning / optimization algorithm for any problem. We often here of how most ML practitioners have a favourite algorithm that they turn to, almost like a silver bullet, for any given machine learning problem such as tree-based ensembling models like XGBoost or LightGBM. After building ML systems for trading strategies at [Plutus Mazu](https://www.plutusmazu.com/), I've become truly humbled by the implications of this theorem. A tree-based ensembling model might work exceptionally well on a large dataset, but with very little data, the most regularized tree-based ensembling model still fails catastrophically during cross-validation as compared to a simple unscaled, KNN for example. Speaking of which, I think this theorem also generalizes to data preprocessing techniques as I've seen that applying scaling for example, is not necessarily always beneficial, especially when you're using an algorithm that computes the distance between features of samples such as KNN, KMeans, ...\n",
    "\n",
    "Because of this, my priors on AutoML have shifted much more positively. Well, then what's the point of hiring a data scientist then? From my knowledge so far, I think that AutoML systems have yet to become \"smarter\". There's an unimaginably huge search space of models to be used in the machine learning pipeline -- which missing data imputer? which dimensionality reduction technique? which scaler to use? which feature selection algorithm? which machine learning algorithm?... There's probably a way to \"bayesianify\" (use bayesian optimization to search over ML models) the AutoML process once we figure out a way to measure similarily across ML techniques on different types of datasets. \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
